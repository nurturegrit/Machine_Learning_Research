{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d15870-616b-4a04-9094-eb0c28b5595b",
   "metadata": {},
   "source": [
    " Let’s take it up a notch. How about we build an end-to-end pipeline for a more complex machine learning project? We can work on creating a predictive model for loan default using the LendingClub dataset.\n",
    "Project Overview\n",
    "\n",
    "The LendingClub dataset contains information about loans issued by the Lending Club. The goal is to build a predictive model that can classify whether a borrower will default on a loan based on their financial and loan-related information.\n",
    "Steps Involved\n",
    "\n",
    "    Understanding the problem\n",
    "    Loading and exploring the data\n",
    "    Preprocessing the data\n",
    "    Exploratory Data Analysis (EDA)\n",
    "    Feature Engineering\n",
    "    Building and training a machine learning model\n",
    "    Evaluating the model\n",
    "    Hyperparameter tuning\n",
    "    Model deployment considerations\n",
    "    Making predictions with the model\n",
    "\n",
    "Dataset Features\n",
    "\n",
    "Some of the key features include:\n",
    "\n",
    "    loan_amnt – Loan amount\n",
    "    term – Term of the loan\n",
    "    int_rate – Interest rate\n",
    "    installment – Installment payment amount\n",
    "    grade – Loan grade\n",
    "    emp_length – Employment length (in years)\n",
    "    home_ownership – Home ownership status\n",
    "    annual_inc – Annual income\n",
    "    purpose – Purpose of the loan\n",
    "    dti – Debt-to-income ratio\n",
    "    delinq_2yrs – Number of delinquent accounts\n",
    "    revol_util – Revolving line utilization rate\n",
    "    total_acc – Total number of credit lines\n",
    "    target – Whether the loan was defaulted (0 for no, 1 for yes)\n",
    "\n",
    "Step 1: Setting Up Your Environment\n",
    "\n",
    "Ensure you have the following libraries installed:\n",
    "\n",
    "pip install pandas numpy scikit-learn matplotlib seaborn xgboost\n",
    "\n",
    "Step 2: Loading and Exploring the Data\n",
    "\n",
    "You can download the LendingClub dataset here. Once you’ve downloaded and extracted it, load the dataset into a Pandas DataFrame.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"LoanStats3b.csv\", low_memory=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Display basic statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "Step 3: Preprocessing the Data\n",
    "\n",
    "We need to handle missing values, encode categorical variables, and standardize numerical features.\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Select relevant features and target\n",
    "features = [\n",
    "    'loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'emp_length', 'home_ownership', \n",
    "    'annual_inc', 'purpose', 'dti', 'delinq_2yrs', 'revol_util', 'total_acc'\n",
    "]\n",
    "target = 'loan_status'  # assuming 'loan_status' indicates default status with 0 for no, 1 for yes\n",
    "\n",
    "df = df[features + [target]]\n",
    "\n",
    "# Removing rows with too many missing values\n",
    "df.dropna(thresh=10, inplace=True)\n",
    "\n",
    "# Handling missing values for numerical features\n",
    "num_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[num_features] = imputer.fit_transform(df[num_features])\n",
    "\n",
    "# Encode categorical features\n",
    "cat_features = df.select_dtypes(include=['object']).columns\n",
    "df = pd.get_dummies(df, columns=cat_features, drop_first=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "Step 4: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualize the data to understand relationships and identify potential features.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of the target variable\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x=target, data=df)\n",
    "plt.title('Target Variable Distribution')\n",
    "plt.show()\n",
    "\n",
    "Step 5: Feature Engineering\n",
    "\n",
    "Create new features and perform any necessary feature transformations.\n",
    "\n",
    "# Feature engineering can include creating new columns for loan amount to annual income ratio, etc.\n",
    "df['loan_income_ratio'] = df['loan_amnt'] / df['annual_inc']\n",
    "\n",
    "# Update features list to include new engineered features\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Update train-test split with new features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "Step 6: Building and Training the Model\n",
    "\n",
    "We will use XGBoost, a powerful gradient boosting framework, for this task.\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create the model pipeline\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameters can be tuned later for better performance\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "Step 7: Evaluating the Model\n",
    "\n",
    "Evaluate the model’s performance using various metrics.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Detailed classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "Step 8: Hyperparameter Tuning\n",
    "\n",
    "Use GridSearchCV to find the best hyperparameters for the XGBoost model.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create the grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3,\n",
    "                           scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "y_best_pred = best_model.predict(X_test)\n",
    "best_accuracy = accuracy_score(y_test, y_best_pred)\n",
    "\n",
    "print(f\"Best Model Accuracy: {best_accuracy}\")\n",
    "\n",
    "Step 9: Model Deployment Considerations\n",
    "\n",
    "Before deploying your model, you should consider aspects such as:\n",
    "\n",
    "    Model versioning\n",
    "    Model explainability\n",
    "    Continuous monitoring\n",
    "    Data drift detection\n",
    "\n",
    "Tools like MLflow, SHAP, and others can be very helpful here.\n",
    "Step 10: Making Predictions\n",
    "\n",
    "Make real-time or batch predictions using your trained model.\n",
    "\n",
    "# Example new data for prediction\n",
    "new_data = X_test.iloc[0:1]\n",
    "\n",
    "# Make prediction\n",
    "prediction = best_model.predict(new_data)\n",
    "print(f\"Predicted Default Status: {'Yes' if prediction[0] == 1 else 'No'}\")\n",
    "\n",
    "Conclusion\n",
    "\n",
    "By now, you have a comprehensive understanding of how to handle a real-world complex machine learning project end-to-end. Feel free to ask questions, or let me know if you’d like to explore specific topics or issues in more detail. Enjoy your advanced machine learning journey!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b2175-793a-4ac3-b0a1-743bd0e3e8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
