{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f98ce5-9fc4-47e4-928f-e6aaa9030f3c",
   "metadata": {},
   "source": [
    "Let’s take on a more advanced machine learning project that incorporates additional layers of complexity with deep learning, natural language processing (NLP), and multi-modal data fusion. We will create an end-to-end system for a sentiment analysis task using a real-world dataset like the Amazon customer reviews. The project will include:\n",
    "\n",
    "    Problem Understanding and Dataset Loading\n",
    "    Data Preprocessing and Cleaning (text and metadata)\n",
    "    Exploratory Data Analysis (EDA)\n",
    "    Feature Engineering\n",
    "    Building and Training Machine Learning Models (NLP and Multimodal)\n",
    "    Evaluating Models\n",
    "    Hyperparameter Tuning\n",
    "    Ensemble Learning\n",
    "    Model Deployment Considerations\n",
    "    Making Predictions\n",
    "\n",
    "Step 1: Setting Up Your Environment\n",
    "\n",
    "Ensure you have the following libraries installed:\n",
    "\n",
    "pip install pandas numpy scikit-learn matplotlib seaborn nltk transformers torch tensorflow\n",
    "\n",
    "Step 2: Loading and Exploring the Data\n",
    "\n",
    "First, let's load the Amazon customer review dataset and explore it. You can download the dataset from Kaggle or Amazon S3 Amazon Customer Reviews Dataset.\n",
    "\n",
    "Here's an example of loading a sample dataset:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming it's a local file for example purposes)\n",
    "# You may need to download and unzip it beforehand\n",
    "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Electronics_v1_00.tsv.gz\"\n",
    "df = pd.read_csv(url, compression='gzip', sep='\\t', error_bad_lines=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Display basic statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "Step 3: Data Preprocessing and Cleaning\n",
    "\n",
    "Here, we’ll preprocess textual data and clean metadata.\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Drop rows that have null values in critical columns\n",
    "df.dropna(subset=['review_body', 'star_rating'], inplace=True)\n",
    "\n",
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([c for c in text if c not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['processed_review'] = df['review_body'].apply(preprocess_text)\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_rating'] = label_encoder.fit_transform(df['star_rating'])\n",
    "\n",
    "# Display preprocessed DataFrame\n",
    "print(df.head())\n",
    "\n",
    "Step 4: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualize the data to understand relationships and identify potential features.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution of the ratings\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x='star_rating', data=df)\n",
    "plt.title('Star Rating Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Word cloud for positive and negative reviews\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate word clouds\n",
    "positive_reviews = ' '.join(df[df['encoded_rating'] == 1]['processed_review'])\n",
    "negative_reviews = ' '.join(df[df['encoded_rating'] == 0]['processed_review'])\n",
    "\n",
    "# Display word clouds\n",
    "wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(positive_reviews)\n",
    "wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"black\").generate(negative_reviews)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "plt.title(\"Positive Reviews Word Cloud\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "plt.title(\"Negative Reviews Word Cloud\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Step 5: Feature Engineering\n",
    "\n",
    "Extract additional features and use embeddings like BERT for NLP tasks.\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Use a pre-trained BERT model to vectorize the reviews\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Apply BERT embeddings\n",
    "df['bert_embedding'] = df['processed_review'].apply(lambda x: get_bert_embeddings(x))\n",
    "\n",
    "# Updating features X and target y for modeling\n",
    "X = np.stack(df['bert_embedding'].values)\n",
    "y = df['encoded_rating']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "Step 6: Building and Training Machine Learning Models\n",
    "\n",
    "We'll use a simple model first, then evolve to more complex architectures like RNNs and transformers.\n",
    "Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "RNN with Keras/TensorFlow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "# Define the RNN model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(LSTM(128, input_shape=(X_train.shape[1], 1), return_sequences=True))\n",
    "rnn_model.add(LSTM(64))\n",
    "rnn_model.add(Dropout(0.3))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the RNN model\n",
    "rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "Step 7: Evaluating Models\n",
    "\n",
    "Evaluate the model’s performance using various metrics.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"ROC AUC: {roc_auc}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# For RandomForest model\n",
    "print(\"Random Forest Model Performance:\")\n",
    "evaluate_model(rf_model, X_test, y_test)\n",
    "\n",
    "# For RNN model, use model.evaluate for performance metrics\n",
    "print(\"RNN Model Performance:\")\n",
    "rnn_performance = rnn_model.evaluate(X_test, y_test)\n",
    "print(f\"RNN Loss: {rnn_performance[0]} - Accuracy: {rnn_performance[1]}\")\n",
    "\n",
    "Step 8: Hyperparameter Tuning\n",
    "\n",
    "Use GridSearchCV or Keras Tuner to find the best hyperparameters.\n",
    "Hyperparameter Tuning for Random Forest\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for RandomForest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Create the grid search\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3,\n",
    "                           scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best RandomForest model\n",
    "print(\"Best Random Forest Model Performance:\")\n",
    "evaluate_model(best_rf_model, X_test, y_test)\n",
    "\n",
    "Step 9: Ensemble Learning\n",
    "\n",
    "Combine multiple models to create an ensemble.\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define individual models\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, max_depth=20, random_state=42)\n",
    "# For RNN, we should wrap Keras model for compatibility with sklearn API\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "rnn_clf = KerasClassifier(build_fn=lambda: rnn_model, epochs=10, batch_size=32)\n",
    "\n",
    "# Define ensemble model\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('rf', rf_clf),\n",
    "    ('rnn', rnn_clf)\n",
    "], voting='soft')\n",
    "\n",
    "# Train ensemble model (on a reduced dataset to avoid long training times)\n",
    "ensemble_model.fit(X_train[:10000], y_train[:10000])\n",
    "\n",
    "# Evaluate ensemble model\n",
    "print(\"Ensemble Model Performance:\")\n",
    "evaluate_model(ensemble_model, X_test[:1000], y_test[:1000])\n",
    "\n",
    "Step 10: Model Deployment Considerations\n",
    "\n",
    "Before deploying your model, consider:\n",
    "\n",
    "    Model versioning\n",
    "    Model explainability\n",
    "    Continuous monitoring\n",
    "    Data drift detection\n",
    "\n",
    "Tools like MLflow, SHAP for explainability, and Flask/Django for API deployments can be useful.\n",
    "Step 11: Making Predictions\n",
    "\n",
    "Make real-time or batch predictions using your trained model.\n",
    "\n",
    "# Example new data for prediction (using X_test sample)\n",
    "new_data = X_test[0:1]\n",
    "\n",
    "# Make prediction\n",
    "prediction = ensemble_model.predict(new_data)\n",
    "probability = ensemble_model.predict_proba(new_data)\n",
    "\n",
    "print(f\"Predicted Sentiment: {'Positive' if prediction[0]==1 else 'Negative'}\")\n",
    "print(f\"Associated Probability: {probability}\")\n",
    "\n",
    "Conclusion\n",
    "\n",
    "In this project, you built a comprehensive end-to-end machine learning pipeline for a sentiment analysis task using multi-modal data. You included advanced techniques like deep learning, NLP with BERT embeddings, handling imbalanced data, ensemble learning, and model evaluation.\n",
    "\n",
    "Feel free to ask for deeper dives into particular sections or for assistance with specific issues. Enjoy advanced machine learning and the challenge of complex real-world app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f0aeb-ea76-49d9-a129-a7b888dd739c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
