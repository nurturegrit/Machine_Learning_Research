{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c124b2b6-964a-4e45-8efa-0d44cb6450ed",
   "metadata": {},
   "source": [
    "et's dive into a more advanced and comprehensive machine learning project. We'll build an end-to-end predictive model for detecting fraudulent transactions using a highly imbalanced dataset like the Credit Card Fraud Detection dataset available on Kaggle.\n",
    "Project Overview\n",
    "\n",
    "The goal is to build a model that accurately identifies fraudulent transactions. This involves dealing with class imbalance, feature selection, and the use of advanced techniques like ensemble learning, feature engineering, and hyperparameter optimization.\n",
    "Steps Involved\n",
    "\n",
    "    Understanding the problem\n",
    "    Loading and exploring the data\n",
    "    Data preprocessing and handling class imbalance\n",
    "    Exploratory Data Analysis (EDA)\n",
    "    Feature Engineering\n",
    "    Building and training machine learning models\n",
    "    Evaluating models\n",
    "    Hyperparameter tuning\n",
    "    Ensemble learning\n",
    "    Model deployment considerations\n",
    "    Making predictions with the model\n",
    "\n",
    "Dataset Features\n",
    "\n",
    "The Credit Card Fraud Detection dataset has the following features:\n",
    "\n",
    "    Time: Number of seconds elapsed between this transaction and the first transaction in the dataset.\n",
    "    V1 to V28: Result of a PCA transformation.\n",
    "    Amount: Transaction amount.\n",
    "    Class: Target variable (0 for non-fraudulent, 1 for fraudulent).\n",
    "\n",
    "Step 1: Setting Up Your Environment\n",
    "\n",
    "Ensure you have the following libraries installed:\n",
    "\n",
    "pip install pandas numpy scikit-learn matplotlib seaborn imbalanced-learn xgboost\n",
    "\n",
    "Step 2: Loading and Exploring the Data\n",
    "\n",
    "First, let's load the dataset and explore it.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://www.kaggle.com/mlg-ulb/creditcardfraud/download\"\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Display basic statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "Step 3: Data Preprocessing and Handling Class Imbalance\n",
    "\n",
    "We'll transform features, scale them, and address class imbalance using SMOTE or undersampling.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Standardize the 'Amount' feature\n",
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Drop 'Time' as it's not informative in this case\n",
    "df.drop(['Time'], axis=1, inplace=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "Step 4: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualize the data to understand relationships and identify potential features.\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution of the target variable\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title('Target Variable Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Pairplot for heavily correlated features\n",
    "plt.figure(figsize=(14, 8))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', robust=True, annot=False, center=0)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "Step 5: Feature Engineering\n",
    "\n",
    "Create interaction features or other meaningful features.\n",
    "\n",
    "# Example Feature Engineering: Interaction features (optional step based on domain knowledge)\n",
    "df['Amount_V1'] = df['Amount'] * df['V1']\n",
    "df['Amount_V2'] = df['Amount'] * df['V2']\n",
    "\n",
    "# Update features list to include new engineered features if applicable\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Update train-test split with new features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "Step 6: Building and Training Machine Learning Models\n",
    "\n",
    "Several models are built and compared. We start with a Random Forest and move to more complex ones.\n",
    "Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "Step 7: Evaluating Models\n",
    "\n",
    "Evaluate the model's performance using various metrics like ROC-AUC, Precision-Recall, etc.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"ROC AUC: {roc_auc}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate RandomForest model\n",
    "print(\"Random Forest Model Performance:\")\n",
    "evaluate_model(rf_model, X_test, y_test)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "print(\"XGBoost Model Performance:\")\n",
    "evaluate_model(xgb_model, X_test, y_test)\n",
    "\n",
    "Step 8: Hyperparameter Tuning\n",
    "\n",
    "Use GridSearchCV to find the best hyperparameters for the chosen model.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create the grid search\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3,\n",
    "                           scoring='roc_auc', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Best Model Performance after Hyperparameter Tuning:\")\n",
    "evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "Step 9: Ensemble Learning\n",
    "\n",
    "Combine multiple models to create a more robust solution.\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define individual models\n",
    "xgb_clf = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define ensemble model\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "    ('xgb', xgb_clf),\n",
    "    ('rf', rf_clf)\n",
    "], voting='soft')\n",
    "\n",
    "# Train ensemble model\n",
    "ensemble_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "print(\"Ensemble Model Performance:\")\n",
    "evaluate_model(ensemble_model, X_test, y_test)\n",
    "\n",
    "Step 10: Model Deployment Considerations\n",
    "\n",
    "Before deploying your model, consider:\n",
    "\n",
    "    Model versioning\n",
    "    Model explainability\n",
    "    Continuous monitoring\n",
    "    Data drift detection\n",
    "\n",
    "Tools like MLflow, SHAP for explainability, and Flask/Django for API deployments are beneficial.\n",
    "Step 11: Making Predictions\n",
    "\n",
    "Make real-time or batch predictions using your trained model.\n",
    "\n",
    "# Example new data for prediction\n",
    "new_data = X_test.iloc[0:1]\n",
    "\n",
    "# Make prediction\n",
    "prediction = ensemble_model.predict(new_data)\n",
    "probability = ensemble_model.predict_proba(new_data)\n",
    "\n",
    "print(f\"Predicted Fraud Status: {'Fraudulent' if prediction[0]==1 else 'Non-Fraudulent'}\")\n",
    "print(f\"Associated Probability: {probability}\")\n",
    "\n",
    "Conclusion\n",
    "\n",
    "By following these steps, you've constructed a highly sophisticated machine learning pipeline for detecting fraudulent transactions. This includes handling class imbalance, feature engineering, building multiple models, hyperparameter tuning, and ensemble learning.\n",
    "\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d9f2c-155c-48a6-b0bd-7f6862a6459b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
